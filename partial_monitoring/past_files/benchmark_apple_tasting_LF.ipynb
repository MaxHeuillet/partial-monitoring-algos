{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import geometry\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from multiprocess import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import geometry\n",
    "import itertools \n",
    "from itertools import islice\n",
    "import games\n",
    "import cpb\n",
    "\n",
    "import cpb_gaussian\n",
    "\n",
    "import bpm\n",
    "import random_algo\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import TSPM\n",
    "import PM_DMED\n",
    "\n",
    "\n",
    "import feedexp3_piccolboni\n",
    "import cpb_gaussian_v2\n",
    "\n",
    "def evaluate_parallel(nbCores, n_folds, horizon, alg, game, type, L):\n",
    "    print(\"nbCores:\", nbCores, \"nbFolds:\", n_folds, \"Horizon:\", horizon)\n",
    "    pool = Pool(processes = nbCores) \n",
    "    task = Evaluation(horizon, type, L)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    distributions = []\n",
    "\n",
    "    for jobid in range(n_folds):\n",
    "        \n",
    "        p = np.random.uniform(0, 0.2) if type == 'easy' else np.random.uniform(0.4,0.5)\n",
    "        distributions.append( [p, 1-p] )\n",
    "\n",
    "\n",
    "    return np.asarray(  pool.map( partial( task.eval_policy_once, alg, game ), zip(distributions ,range(n_folds)) ) ) \n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "\n",
    "    def __init__(self, horizon,type, L):\n",
    "        self.type = type\n",
    "        self.horizon = horizon\n",
    "        self.L = L\n",
    "        # self.outcome_distribution = outcome_distribution\n",
    "\n",
    "    \n",
    "\n",
    "    def get_outcomes(self, game, job_id):\n",
    "        # self.means = runif_in_simplex( self.game.n_outcomes )\n",
    "        outcomes = np.random.choice( game.n_outcomes , p= list( game.outcome_dist.values() ), size= self.horizon) \n",
    "        return outcomes\n",
    "\n",
    "    def get_feedback(self, game, action, outcome):\n",
    "        return game.FeedbackMatrix[ action ][ outcome ]\n",
    "\n",
    "    def eval_policy_once(self, alg, game, job):\n",
    "\n",
    "        alg.reset()\n",
    "\n",
    "        distribution, jobid = job\n",
    "\n",
    "        np.random.seed(jobid)\n",
    "        \n",
    "        # outcome_distribution =  {'spam':0.05,'ham':0.95}\n",
    "\n",
    "        outcome_distribution =  {'spam':distribution[0],'ham':distribution[1]}\n",
    "\n",
    "        # p = get_easy() if game.mode == 'easy' else get_harsch() \n",
    "        # outcome_distribution =  {'a':p[0],'b':p[1],'c':p[2],'d':p[3],'e':p[4]}\n",
    "\n",
    "        game.set_outcome_distribution( outcome_distribution )\n",
    "        # print('optimal action', game.i_star)\n",
    "\n",
    "        action_counter = np.zeros( (game.n_actions, self.horizon) )\n",
    "\n",
    "        # generate outcomes obliviously\n",
    "        outcomes = self.get_outcomes(game, jobid)\n",
    "\n",
    "        buffer = []\n",
    "\n",
    "        for t in range(self.horizon):\n",
    "\n",
    "            # policy chooses one action\n",
    "            action = alg.get_action(t)\n",
    "\n",
    "            # Environment chooses one outcome\n",
    "            outcome = outcomes[t]\n",
    "\n",
    "            # p r i n t ( ' t ' , t , ' a c t i o n ' , a c t i o n , ' o u t c o m e ' , o u t c o m e , )\n",
    "\n",
    "            feedback =  self.get_feedback( game, action, outcome )\n",
    "\n",
    "            buffer.append( [action, outcome, feedback] )\n",
    "\n",
    "            if len(buffer)>=self.L or t<game.N:\n",
    "                for data in buffer:\n",
    "                    action, outcome, feedback = data \n",
    "                    alg.update(action, feedback, outcome, None, t)\n",
    "                buffer = []\n",
    "            \n",
    "            # p r i n t ( ' n u ' , a l g . n u / a l g . n )\n",
    "\n",
    "            for i in range(game.n_actions):\n",
    "                if i == action:\n",
    "                    action_counter[i][t] = action_counter[i][t-1] + 1\n",
    "                else:\n",
    "                    \n",
    "                    action_counter[i][t] = action_counter[i][t-1]\n",
    "\n",
    "        regret = np.array( [ game.delta(i) for i in range(game.n_actions) ]).T @ action_counter\n",
    "\n",
    "        return regret\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "n_cores = 1\n",
    "n_folds = 1\n",
    "horizon = 1000\n",
    "\n",
    "# np.seterr(all='raise')\n",
    "\n",
    "# game = games.apple_tasting(False, outcome_distribution) \n",
    "\n",
    "game =  games.apple_tasting( False ) \n",
    "\n",
    "outcome_distribution = [0.2,0.8]\n",
    "job = (outcome_distribution, 1 )\n",
    "\n",
    "# print('optimal action', game.i_star)\n",
    "\n",
    "alg = cpb.CPB(  game, horizon, 1.01) #TSPM.TSPM_alg(  game, horizon, 1)\n",
    "task = Evaluation( horizon, 'easy', 100)\n",
    "\n",
    "result = task.eval_policy_once(alg,game, job)\n",
    "plt.plot(range(horizon), result)\n",
    "# fig = go.Figure( )\n",
    "# regret = np.array([ game.delta(i) for i in range(game.n_actions) ]).T @ np.mean(result,0) \n",
    "# xcoords = np.arange(0,horizon,1).tolist()\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='blue'), mode='lines',  name='TPSM' )) # \n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd7ea465340>]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "\n",
    "#import ucbTSPM_v2\n",
    "n_cores = 16\n",
    "n_folds = 100\n",
    "horizon = 2500\n",
    "# outcome_distribution =  {'spam':0.05,'ham':0.95}\n",
    "\n",
    "game = games.apple_tasting(False)\n",
    "\n",
    "\n",
    "#feedexp3.FeedExp3(  game, horizon, ),\n",
    "#feedexp3_v3.FeedExp3(  game, horizon, ),\n",
    "#eTSPM.eTSPM_alg(  game, horizon, 1),\n",
    "#TSPM.TSPM_alg(  game, horizon,), bpm.BPM(  game, horizon,  [0.5, 0.5 ], np.identity(2) ) ,\n",
    "\n",
    "#TSPM.TSPM_alg(  game, horizon,), \n",
    "#ucbTSPM_v2.TSPM_alg(game, horizon)  \n",
    "\n",
    "algos = [   random_algo.Random(  game, horizon, ),\n",
    "\n",
    "            feedexp3_piccolboni.FeedExp3(  game, horizon, ),\n",
    "\n",
    "            cpb.CPB(  game, horizon, 1.01), \n",
    "            cpb_gaussian.CPB_gaussian(  game, horizon, 1.01, True, 1/16, 10), \n",
    "\n",
    "            PM_DMED.PM_DMED(  game, horizon,),   \n",
    "\n",
    "            TSPM.TSPM_alg(  game, horizon, 1),\n",
    "            TSPM.TSPM_alg(  game, horizon, 0), \n",
    "\n",
    "            bpm.BPM(game,horizon),  ] \n",
    "\n",
    "colors = [  [0,0,0], [250,0,0], [0,250,0], [0,125,0], [250,0,250], [0,0,250], [0,0,125],  [0,125,125]  ]\n",
    "labels = [   'random', 'Piccolboni','CBP', 'RandCBP', 'PM_DMED', 'TSPM_R1', 'TSPM_R0', 'BPM_LEAST'   ] \n",
    "\n",
    "fig = go.Figure(    )\n",
    "\n",
    "for alg, color, label in zip( algos, colors, labels):\n",
    "\n",
    "    r,g,b = color\n",
    "    result = evaluate_parallel(n_cores, n_folds, horizon, alg, game, 'hard')\n",
    "    np.save('./results/apple_tasting/hard_{}_{}_{}'.format(horizon,n_folds, label), result)\n",
    "    regret =  np.mean(result,0) \n",
    "    xcoords = np.arange(0,horizon,1).tolist()\n",
    "    std =  np.std(result,0) \n",
    "    upper_regret = regret + std\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='rgb({},{},{})'.format(r,g,b)), mode='lines',  name=label )) # \n",
    "\n",
    "    fig.add_trace(   go.Scatter( x=xcoords+xcoords[::-1], y=upper_regret.tolist()+regret.tolist()[::-1],  fill='toself', fillcolor='rgba({},{},{},0.2)'.format(r,g,b), \n",
    "                         line=dict(color='rgba(255,255,255,0)'),   hoverinfo=\"skip\",  showlegend=False )\n",
    "\n",
    "    )\n",
    "    \n",
    "fig.show(legend=True)\n",
    "fig.update_yaxes(range=[0, 100] )\n",
    "\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                 xaxis_title=\"Sequence\",\n",
    "                 yaxis_title=\"Regret\",\n",
    "                 margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ), \n",
    "                  font=dict(size=13,) )\n",
    "fig.write_image(\"./hard_AT_LF.pdf\")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Restricted license - for non-production use only - expires 2023-10-25\n",
      "weights [0. 1.]\n",
      "n-actions 2 n-outcomes 2 alphabet 2\n",
      "n-actions 2 n-outcomes 2 alphabet 2\n",
      "n-actions 2 n-outcomes 2 alphabet 2\n",
      "n-actions 2 n-outcomes 2 alphabet 2\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "evaluate_parallel() missing 1 required positional argument: 'L'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alg, color, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m( algos, colors, labels):\n\u001b[1;32m     39\u001b[0m     r,g,b \u001b[38;5;241m=\u001b[39m color\n\u001b[0;32m---> 40\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_cores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/apple_tasting/hard_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(horizon,n_folds, label), result)\n\u001b[1;32m     42\u001b[0m     regret \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mmean(result,\u001b[38;5;241m0\u001b[39m) \n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_parallel() missing 1 required positional argument: 'L'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import ucbTSPM_v2\n",
    "import cpb_gaussian_v2\n",
    "\n",
    "n_cores = 16\n",
    "n_folds = 100\n",
    "horizon = 2500\n",
    "# outcome_distribution =  {'spam':0.05,'ham':0.95}\n",
    "\n",
    "game = games.apple_tasting(False)\n",
    "\n",
    "algos = [   random_algo.Random(  game, horizon, ),\n",
    "\n",
    "            feedexp3_piccolboni.FeedExp3(  game, horizon, ),\n",
    "\n",
    "            cpb.CPB(  game, horizon, 1.01), \n",
    "            cpb_gaussian.CPB_gaussian(  game, horizon, 1.01, True, 1/16, 10), \n",
    "\n",
    "            PM_DMED.PM_DMED(  game, horizon,),   \n",
    "\n",
    "            TSPM.TSPM_alg(  game, horizon, 1),\n",
    "            TSPM.TSPM_alg(  game, horizon, 0), \n",
    "\n",
    "            bpm.BPM(game,horizon),  ] \n",
    "\n",
    "colors = [  [0,0,0], [250,0,0], [0,250,0], [0,125,0], [250,0,250], [0,0,250], [0,0,125],  [0,125,125]  ]\n",
    "labels = [   'random', 'Piccolboni','CBP', 'RandCBP', 'PM_DMED', 'TSPM_R1', 'TSPM_R0', 'BPM_LEAST'   ] \n",
    "\n",
    "experiment_results = {}\n",
    "\n",
    "for L in [1, 3, 10, 32, 100, 316, 1000] :\n",
    "  \n",
    "  fig = go.Figure( )\n",
    "  final_regrets = []\n",
    "  perfs = []\n",
    "\n",
    "  for alg, color, label in zip( algos, colors, labels):\n",
    "    r,g,b = color\n",
    "    result = evaluate_parallel(n_cores, n_folds, horizon, alg, game, 'easy', L)\n",
    "    np.save('./results/apple_tasting/easy_{}_{}_{}'.format(horizon,n_folds, label), result)\n",
    "    final_regrets.append( result[:,-1] )\n",
    "    regret =  np.mean(result, 0) \n",
    "    xcoords = np.arange(0,horizon,1).tolist()\n",
    "    std =  np.std(result,0) \n",
    "    upper_regret = regret + std\n",
    "    perfs.append( (regret[-1], std[-1]) )\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='rgb({},{},{})'.format(r,g,b)), mode='lines',  name=label )) # \n",
    "\n",
    "    fig.add_trace(   go.Scatter( x=xcoords+xcoords[::-1], y=upper_regret.tolist()+regret.tolist()[::-1],  fill='toself', fillcolor='rgba({},{},{},0.2)'.format(r,g,b), \n",
    "                                        line=dict(color='rgba(255,255,255,0)'),   hoverinfo=\"skip\",  showlegend=False )  )\n",
    "        \n",
    "  #fig.show(legend=True)\n",
    "  fig.update_yaxes(range=[0, 30] )\n",
    "  fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                        xaxis_title=\"Sequence\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "  fig.write_image(\"./easy_LE_{}.pdf\".format(L) )\n",
    "\n",
    "  #fig.show()\n",
    "\n",
    "  final_regrets = np.array(final_regrets)\n",
    "  final = [ ( np.argmin(final_regrets[:,i]), i) for i in range(n_folds) ]\n",
    "\n",
    "  experiment_results[L] = perfs\n",
    "\n",
    "print(experiment_results)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0215ed8d73c40df5de54d647d65b604dcf02460a7de2b27ed9878602c67cf72c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('climate-ai': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}