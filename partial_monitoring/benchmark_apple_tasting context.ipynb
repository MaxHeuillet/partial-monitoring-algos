{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocess import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import games\n",
    "\n",
    "import cpb\n",
    "# import cpb_uniform\n",
    "import cpb_gaussian\n",
    "\n",
    "import random_algo\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import synthetic_data\n",
    "import cpb_side\n",
    "import cpb_side_gaussian\n",
    "\n",
    "\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "\n",
    "import linucb\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_parallel(nbCores, n_folds, horizon, alg, game, type):\n",
    "    print(\"nbCores:\", nbCores, \"nbFolds:\", n_folds, \"Horizon:\", horizon)\n",
    "    pool = Pool(processes = nbCores) \n",
    "    task = Evaluation(horizon, type)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    distributions = []\n",
    "    context_generators = []\n",
    "\n",
    "    for jobid in range(n_folds):\n",
    "        \n",
    "        p = np.random.uniform(0, 0.2) if type == 'easy' else np.random.uniform(0.4,0.5)\n",
    "        distributions.append( [p, 1-p] )\n",
    "\n",
    "        d = 2\n",
    "        margin =0.01\n",
    "        contexts = synthetic_data.LinearContexts( np.array([0.5,0.5]), 0, d, margin) #synthetic_data.ToyContexts( )\n",
    "        context_generators.append( contexts )\n",
    "\n",
    "    return np.asarray(  pool.map( partial( task.eval_policy_once, alg, game ), zip(distributions , context_generators ,range(n_folds)) ) ) \n",
    "\n",
    "class Evaluation:\n",
    "\n",
    "    def __init__(self, horizon,type ):\n",
    "        self.type = type\n",
    "        self.horizon = horizon\n",
    "        # self.outcome_distribution = outcome_distribution\n",
    "\n",
    "    def get_outcomes(self, game, job_id):\n",
    "        # self.means = runif_in_simplex( self.game.n_outcomes )\n",
    "        outcomes = np.random.choice( game.n_outcomes , p= list( game.outcome_dist.values() ), size= self.horizon) \n",
    "        return outcomes\n",
    "\n",
    "    def get_feedback(self, game, action, outcome):\n",
    "        return game.FeedbackMatrix[ action ][ outcome ]\n",
    "\n",
    "    def get_bandit_feedback(self, game, action, outcome):\n",
    "        return game.banditFeedbackMatrix[ action ][ outcome ]\n",
    "\n",
    "    def eval_policy_once(self, alg, game, job):\n",
    "\n",
    "        distribution, context_generator, jobid = job\n",
    "\n",
    "        np.random.seed(jobid)\n",
    "\n",
    "        # outcome_distribution =  {'spam':0.5,'ham':0.5}\n",
    "        outcome_distribution =  {'spam':distribution[0],'ham':distribution[1]}\n",
    "\n",
    "        game.set_outcome_distribution( outcome_distribution, jobid )\n",
    "        #print('optimal action', game.i_star)\n",
    "\n",
    "        # action_counter = np.zeros( (game.n_actions, self.horizon) )\n",
    "\n",
    "        # generate outcomes obliviously\n",
    "        outcomes = self.get_outcomes(game, jobid)\n",
    "        # contexts = [ context_generator.get_context(outcome) for outcome in outcomes ]\n",
    "        context_generator.generate_unique_context()\n",
    "        contexts = [ context_generator.get_same_context(outcome) for outcome in outcomes ]\n",
    "\n",
    "        cumRegret =  np.zeros(self.horizon, dtype =float)\n",
    "\n",
    "        for t in range(self.horizon):\n",
    "\n",
    "            # Environment chooses one outcome and one context associated to this outcome\n",
    "            outcome = outcomes[t]\n",
    "            context = contexts[t]\n",
    "\n",
    "            # print(context.T.shape)\n",
    "            # policy chooses one action\n",
    "            # print('t', t,  'outcome', outcome, 'context', context)\n",
    "            action = alg.get_action(t, context)\n",
    "            # print('t', t, 'action', action, 'outcome', outcome, 'context', context)\n",
    "            \n",
    "            feedback =  self.get_feedback( game, action, outcome )\n",
    "            bandit_feedback =  self.get_bandit_feedback( game, action, outcome )\n",
    "\n",
    "            alg.update(action, feedback, bandit_feedback, outcome, t, context )\n",
    "            \n",
    "            # print('nu', alg.nu / alg.n )\n",
    "            regret = game.LossMatrix[action, outcome] - np.min( game.LossMatrix[...,outcome] )\n",
    "            # print( 'regret:' , regret )\n",
    "            cumRegret[t] =  regret\n",
    "            # print()\n",
    "            # print()\n",
    "        # regret = np.array( [ game.delta(i) for i in range(game.n_actions) ]).T @ action_counter\n",
    "        #context_regret = np.cumsum( \n",
    "        # cumRegret )\n",
    "\n",
    "        alg.reset()\n",
    "\n",
    "        return  np.cumsum( cumRegret ) #regret"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import PGIDSratio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "# n_cores = 1\n",
    "# n_folds = 1\n",
    "# horizon = 100\n",
    "\n",
    "#game =  games.apple_tasting( False ) \n",
    "game = games.apple_tasting(False)\n",
    "\n",
    "\n",
    "alg = PGIDSratio.PGIDSratio( game, horizon, 2 )\n",
    "# task = Evaluation(horizon, 'easy')\n",
    "\n",
    "# outcome_distribution = [0.5, 0.5]\n",
    "# d = 2\n",
    "# margin = 0.01\n",
    "# contexts_generator = synthetic_data.LinearContexts( np.array([0.5,0.5]), 0, d, margin) #synthetic_data.ToyContexts( ) #\n",
    "# job = (outcome_distribution, contexts_generator, 3 )\n",
    "\n",
    "# result = task.eval_policy_once(alg, game, job)\n",
    "\n",
    "n_cores = 8\n",
    "n_folds = 8\n",
    "horizon = 1000\n",
    "\n",
    "result = evaluate_parallel(n_cores, n_folds, horizon, alg, game, 'easy')\n",
    "regret =  np.mean(result, 0) \n",
    "xcoords = np.arange(0,horizon,1).tolist()\n",
    "std =  np.std(result,0) \n",
    "\n",
    "plt.plot( regret )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import PM_DMED\n",
    "\n",
    "n_cores = 1\n",
    "n_folds = 1\n",
    "horizon = 100\n",
    "\n",
    "# np.seterr(all='raise')\n",
    "\n",
    "# game = games.apple_tasting(False, outcome_distribution) \n",
    "\n",
    "outcome_distribution = [0.8,0.2]\n",
    "job = (outcome_distribution, 1 )\n",
    "\n",
    "\n",
    "game =  games.label_efficient(  ) \n",
    "game.set_outcome_distribution( {'spam':outcome_distribution[0],'ham':outcome_distribution[1]} )\n",
    "print('optimal action', game.i_star)\n",
    "\n",
    "\n",
    "# print('optimal action', game.i_star)\n",
    "alg = cpb.CPB(  game, horizon,1.01) #TSPM.TSPM_alg(  game, horizon, 1)\n",
    "task = Evaluation(horizon, 'easy')\n",
    "\n",
    "result = task.eval_policy_once(alg,game, job)\n",
    "#plt.plot(range(horizon), result)\n",
    "# fig = go.Figure( )\n",
    "# regret = np.array([ game.delta(i) for i in range(game.n_actions) ]).T @ np.mean(result,0) \n",
    "# xcoords = np.arange(0,horizon,1).tolist()\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='blue'), mode='lines',  name='TPSM' )) # \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_cores = 8\n",
    "n_folds = 50\n",
    "horizon = 10000\n",
    "# outcome_distribution =  {'spam':0.05,'ham':0.95}\n",
    "\n",
    "game = games.label_efficient(  )\n",
    " \n",
    "algos = [ PGIDSratio.PGIDSratio( game, horizon, 2 )   ]\n",
    "\n",
    "\n",
    "colors = [  [0,0,0], [250,0,0], [200,0,0], [0,200,0]   ] #[0,150,0], [0,250,0], [0,150,0], [0,0,250], [0,0,0],  [0,255,0], [0 , 150, 0], [155,155,0], [255,0,0], [0,0,255] , [255,51,255], [255,51,255], [255,20,200]  ] #\n",
    "labels = [  'random',  'CPB',  'RandCPB', 'TSPM'   ]  # 'random', 'TSPM' , 'TSPM (R=0)', 'RandCBP (uncoupled)','CPB uniform', ,'TSPM' , 'ucbTSPM (Auer)' 'FeedExp3 (2001)', 'FeedExp3 (2006)', 'CPB',  'eTSPM (Auer)',\n",
    "\n",
    "fig = go.Figure( )\n",
    "\n",
    "final_regrets = []\n",
    "\n",
    "for alg, color, label in zip( algos, colors, labels):\n",
    "\n",
    "    r,g,b = color\n",
    "    result = evaluate_parallel(n_cores, n_folds, horizon, alg, game, 'harsch')\n",
    "    final_regrets.append( result[:,-1] )\n",
    "    regret =  np.mean(result, 0) \n",
    "    xcoords = np.arange(0,horizon,1).tolist()\n",
    "    std =  np.std(result,0) \n",
    "    upper_regret = regret + std\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='rgb({},{},{})'.format(r,g,b)), mode='lines',  name=label )) # \n",
    "\n",
    "    fig.add_trace(   go.Scatter( x=xcoords+xcoords[::-1], y=upper_regret.tolist()+regret.tolist()[::-1],  fill='toself', fillcolor='rgba({},{},{},0.2)'.format(r,g,b), \n",
    "                         line=dict(color='rgba(255,255,255,0)'),   hoverinfo=\"skip\",  showlegend=False )  )\n",
    "    \n",
    "fig.show(legend=True)\n",
    "fig.update_yaxes(range=[0, 30] )\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                  xaxis_title=\"Sequence\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "fig.write_image(\"./hard_LE.pdf\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "final_regrets = np.array(final_regrets)\n",
    "final = [ ( np.argmin(final_regrets[:,i]), i) for i in range(n_folds) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig.update_xaxes(type=\"linear\")\n",
    "fig.update_yaxes(range=[0, 12000] )\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                  xaxis_title=\"Sequence\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "fig.write_image(\"./hard_LE_log.pdf\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_cores = 8\n",
    "n_folds = 200\n",
    "horizon = 5000\n",
    "# outcome_distribution =  {'spam':0.05,'ham':0.95}\n",
    "\n",
    "game = games.label_efficient(  )\n",
    "\n",
    "#feedexp3.FeedExp3(  game, horizon, ),\n",
    "#feedexp3_v3.FeedExp3(  game, horizon, ),\n",
    "#eTSPM.eTSPM_alg(  game, horizon, 1),\n",
    "#TSPM.TSPM_alg(  game, horizon,), bpm.BPM(  game, horizon,  [0.5, 0.5 ], np.identity(2) ) ,\n",
    "\n",
    "#TSPM.TSPM_alg(  game, horizon,), \n",
    "#ucbTSPM_v2.TSPM_alg(game, horizon)  \n",
    "\n",
    "algos = [ random_algo.Random(  game, horizon, ),      \n",
    "        cpb.CPB(  game, horizon, 1.01), \n",
    "        cpb_gaussian.CPB_gaussian(  game, horizon, 1.01, True, 1/16, 10),\n",
    "        TSPM.TSPM_alg(  game, horizon, 1),   ] \n",
    "\n",
    "        #cpb_gaussian_v2.CPB_gaussian(  game, horizon, 1.01, True), \n",
    "        #cpb_gaussian_v2.CPB_gaussian(  game, horizon, 1.01, False), ]\n",
    "        #cpb_gaussian_v2.CPB_gaussian(  game, horizon, 1.01),\n",
    "        #TSPM.TSPM_alg(game, horizon, 1 ) ]\n",
    "        #TSPM.TSPM_alg(game, horizon, 0 )   #eTSPM.eTSPM_alg(game, horizon, 1), cpb_uniform.CPB_uniform(  game, horizon, 1.01), \n",
    "\n",
    "colors = [  [0,0,0], [250,0,0], [200,0,0], [0,200,0]   ] #[0,150,0], [0,250,0], [0,150,0], [0,0,250], [0,0,0],  [0,255,0], [0 , 150, 0], [155,155,0], [255,0,0], [0,0,255] , [255,51,255], [255,51,255], [255,20,200]  ] #\n",
    "labels = [  'random',  'CPB',  'RandCPB', 'TSPM'   ]  # 'random', 'TSPM' , 'TSPM (R=0)', 'RandCBP (uncoupled)','CPB uniform', ,'TSPM' , 'ucbTSPM (Auer)' 'FeedExp3 (2001)', 'FeedExp3 (2006)', 'CPB',  'eTSPM (Auer)',\n",
    "\n",
    "fig = go.Figure( )\n",
    "\n",
    "final_regrets = []\n",
    "\n",
    "for alg, color, label in zip( algos, colors, labels):\n",
    "\n",
    "    r,g,b = color\n",
    "    result = evaluate_parallel(n_cores, n_folds, horizon, alg, game, 'easy')\n",
    "    final_regrets.append( result[:,-1] )\n",
    "    regret =  np.mean(result, 0) \n",
    "    xcoords = np.arange(0,horizon,1).tolist()\n",
    "    std =  np.std(result,0) \n",
    "    upper_regret = regret + std\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='rgb({},{},{})'.format(r,g,b)), mode='lines',  name=label )) # \n",
    "\n",
    "    fig.add_trace(   go.Scatter( x=xcoords+xcoords[::-1], y=upper_regret.tolist()+regret.tolist()[::-1],  fill='toself', fillcolor='rgba({},{},{},0.2)'.format(r,g,b), \n",
    "                         line=dict(color='rgba(255,255,255,0)'),   hoverinfo=\"skip\",  showlegend=False )  )\n",
    "    \n",
    "fig.show(legend=True)\n",
    "fig.update_yaxes(range=[0, 30] )\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                  xaxis_title=\"Sequence\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "fig.write_image(\"./easy_LE.pdf\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "final_regrets = np.array(final_regrets)\n",
    "final = [ ( np.argmin(final_regrets[:,i]), i) for i in range(n_folds) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "res = []\n",
    "for i in range(n_folds):\n",
    "    opt = min(final_regrets[:,i])\n",
    "    vec = [ j == opt for j in final_regrets[:,i] ] \n",
    "    #if vec[1]==1 and vec[2] == 1:\n",
    "    #    pass\n",
    "    #else:\n",
    "    res.append(  vec  )\n",
    "\n",
    "#print( np.sum( res,0) )\n",
    "\n",
    "diff = []\n",
    "for i in range(n_folds):\n",
    "    if res[i][1] >= res[i][2]:\n",
    "        diff.append( i )\n",
    "\n",
    "np.random.seed(1)\n",
    "distributions = []\n",
    "for jobid in range(n_folds):\n",
    "    p = np.random.uniform(0.4, 0.5) \n",
    "    distributions.append( [p, 1-p] )\n",
    "\n",
    "distributions_rand = np.array([ distributions[i] for i in diff ])\n",
    "distributions_cbp = np.array([ distributions[i] for i in range(n_folds) if i not in diff ])\n",
    "#print( len( diff ) )\n",
    "print(np.mean(distributions_rand[:,0]), np.std(distributions_rand[:,0]) )\n",
    "print(np.mean(distributions_cbp[:,0]), np.std(distributions_cbp[:,0]) )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.456154090331781 0.03132638699822154\n",
      "0.43466197585846006 0.024706747598054565\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                  xaxis_title=\"Sequence (log-scale)\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "fig.write_image(\"./easy_LE_log.pdf\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "fig.write_image(\"./easy_LE.pdf\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0215ed8d73c40df5de54d647d65b604dcf02460a7de2b27ed9878602c67cf72c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.15 64-bit ('climate-ai': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}