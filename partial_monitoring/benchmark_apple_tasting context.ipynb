{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocess import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import games\n",
    "\n",
    "import cpb\n",
    "# import cpb_uniform\n",
    "import cpb_gaussian\n",
    "\n",
    "import random_algo\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import synthetic_data\n",
    "import cpb_side\n",
    "import cpb_side_gaussian\n",
    "\n",
    "\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "\n",
    "import linucb\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_parallel(nbCores, n_folds, horizon, alg, game, type):\n",
    "    print(\"nbCores:\", nbCores, \"nbFolds:\", n_folds, \"Horizon:\", horizon)\n",
    "    pool = Pool(processes = nbCores) \n",
    "    task = Evaluation(horizon, type)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    distributions = []\n",
    "    context_generators = []\n",
    "\n",
    "    for jobid in range(n_folds):\n",
    "        \n",
    "        p = np.random.uniform(0, 0.2) if type == 'easy' else np.random.uniform(0.4,0.5)\n",
    "        distributions.append( [p, 1-p] )\n",
    "\n",
    "        d = 2\n",
    "        margin =0.01\n",
    "        contexts = synthetic_data.LinearContexts( np.array([0.5,0.5]), 0, d, margin) #synthetic_data.ToyContexts( )\n",
    "        context_generators.append( contexts )\n",
    "\n",
    "    return np.asarray(  pool.map( partial( task.eval_policy_once, alg, game ), zip(distributions , context_generators ,range(n_folds)) ) ) \n",
    "\n",
    "class Evaluation:\n",
    "\n",
    "    def __init__(self, horizon,type ):\n",
    "        self.type = type\n",
    "        self.horizon = horizon\n",
    "        # self.outcome_distribution = outcome_distribution\n",
    "\n",
    "    def get_outcomes(self, game, job_id):\n",
    "        # self.means = runif_in_simplex( self.game.n_outcomes )\n",
    "        outcomes = np.random.choice( game.n_outcomes , p= list( game.outcome_dist.values() ), size= self.horizon) \n",
    "        return outcomes\n",
    "\n",
    "    def get_feedback(self, game, action, outcome):\n",
    "        return game.FeedbackMatrix[ action ][ outcome ]\n",
    "\n",
    "    def get_bandit_feedback(self, game, action, outcome):\n",
    "        return game.banditFeedbackMatrix[ action ][ outcome ]\n",
    "\n",
    "    def eval_policy_once(self, alg, game, job):\n",
    "\n",
    "        distribution, context_generator, jobid = job\n",
    "\n",
    "        np.random.seed(jobid)\n",
    "\n",
    "        # outcome_distribution =  {'spam':0.5,'ham':0.5}\n",
    "        outcome_distribution =  {'spam':distribution[0],'ham':distribution[1]}\n",
    "\n",
    "        game.set_outcome_distribution( outcome_distribution, jobid )\n",
    "        #print('optimal action', game.i_star)\n",
    "\n",
    "        # action_counter = np.zeros( (game.n_actions, self.horizon) )\n",
    "\n",
    "        # generate outcomes obliviously\n",
    "        outcomes = self.get_outcomes(game, jobid)\n",
    "        # contexts = [ context_generator.get_context(outcome) for outcome in outcomes ]\n",
    "        context_generator.generate_unique_context()\n",
    "        contexts = [ context_generator.get_same_context(outcome) for outcome in outcomes ]\n",
    "\n",
    "        cumRegret =  np.zeros(self.horizon, dtype =float)\n",
    "\n",
    "        for t in range(self.horizon):\n",
    "\n",
    "            # Environment chooses one outcome and one context associated to this outcome\n",
    "            outcome = outcomes[t]\n",
    "            context = contexts[t]\n",
    "\n",
    "            # print(context.T.shape)\n",
    "            # policy chooses one action\n",
    "            # print('t', t,  'outcome', outcome, 'context', context)\n",
    "            action = alg.get_action(t, context)\n",
    "            # print('t', t, 'action', action, 'outcome', outcome, 'context', context)\n",
    "            \n",
    "            feedback =  self.get_feedback( game, action, outcome )\n",
    "            bandit_feedback =  self.get_bandit_feedback( game, action, outcome )\n",
    "\n",
    "            alg.update(action, feedback, bandit_feedback, outcome, t, context )\n",
    "            \n",
    "            # print('nu', alg.nu / alg.n )\n",
    "            regret = game.LossMatrix[action, outcome] - np.min( game.LossMatrix[...,outcome] )\n",
    "            # print( 'regret:' , regret )\n",
    "            cumRegret[t] =  regret\n",
    "            # print()\n",
    "            # print()\n",
    "        # regret = np.array( [ game.delta(i) for i in range(game.n_actions) ]).T @ action_counter\n",
    "        #context_regret = np.cumsum( \n",
    "        # cumRegret )\n",
    "\n",
    "        alg.reset()\n",
    "\n",
    "        return  np.cumsum( cumRegret ) #regret"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import PGIDSratio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "n_cores = 1\n",
    "n_folds = 1\n",
    "horizon = 100\n",
    "\n",
    "game =  games.apple_tasting( False ) \n",
    "game = games.apple_tasting(False)\n",
    "\n",
    "\n",
    "alg = PGIDSratio.PGIDSratio( game, horizon, 2 )\n",
    "task = Evaluation(horizon, 'easy')\n",
    "\n",
    "outcome_distribution = [0.5, 0.5]\n",
    "d = 2\n",
    "margin = 0.01\n",
    "contexts_generator = synthetic_data.LinearContexts( np.array([0.5,0.5]), 0, d, margin) #synthetic_data.ToyContexts( ) #\n",
    "job = (outcome_distribution, contexts_generator, 3 )\n",
    "\n",
    "result = task.eval_policy_once(alg, game, job)\n",
    "\n",
    "# n_cores = 8\n",
    "# n_folds = 8\n",
    "# horizon = 1000\n",
    "\n",
    "# result = evaluate_parallel(n_cores, n_folds, horizon, alg, game, 'easy')\n",
    "# regret =  np.mean(result, 0) \n",
    "# xcoords = np.arange(0,horizon,1).tolist()\n",
    "# std =  np.std(result,0) \n",
    "\n",
    "# plt.plot( regret )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.94400913 0.04103379]\n",
      " [0.04103379 0.96992775]] real [[0.94400913 0.04103379]\n",
      " [0.04103379 0.96992775]]\n",
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.91865451 0.05961531]\n",
      " [0.05961531 0.95630999]] real [[0.91865451 0.05961531]\n",
      " [0.05961531 0.95630999]]\n",
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.91957625 0.0589398 ]\n",
      " [0.0589398  0.95680505]] real [[0.91957625 0.0589398 ]\n",
      " [0.0589398  0.95680505]]\n",
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.97036034 0.02172189]\n",
      " [0.02172189 0.98408077]] real [[0.97036034 0.02172189]\n",
      " [0.02172189 0.98408077]]\n",
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.83778401 0.11888252]\n",
      " [0.11888252 0.91287509]] real [[0.83778401 0.11888252]\n",
      " [0.11888252 0.91287509]]\n",
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.91792308 0.06015135]\n",
      " [0.06015135 0.95591714]] real [[0.91792308 0.06015135]\n",
      " [0.06015135 0.95591714]]\n",
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.96945139 0.02238803]\n",
      " [0.02238803 0.98359258]] real [[0.96945139 0.02238803]\n",
      " [0.02238803 0.98359258]]\n",
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.85180103 0.10860993]\n",
      " [0.10860993 0.92040352]] real [[0.85180103 0.10860993]\n",
      " [0.10860993 0.92040352]]\n",
      "features (1, 2) omega matrix (1, 1) both (2, 1)\n",
      "test [[0.97073395 0.02144808]\n",
      " [0.02144808 0.98428144]] real [[0.97073395 0.02144808]\n",
      " [0.02144808 0.98428144]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.70953009 0.1854926 ]\n",
      " [0.21287541 0.86405885]] real [[0.77605351 0.16412268]\n",
      " [0.16412268 0.87972013]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.79202548 0.18558007]\n",
      " [0.15241737 0.86399475]] real [[0.80572012 0.14238104]\n",
      " [0.14238104 0.89565383]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.91761814 0.06330873]\n",
      " [0.06037483 0.95360321]] real [[0.92079756 0.05804475]\n",
      " [0.05804475 0.957461  ]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.87510761 0.08120956]\n",
      " [0.09152934 0.9404843 ]] real [[0.8922899  0.07893703]\n",
      " [0.07893703 0.94214977]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.90622502 0.0720187 ]\n",
      " [0.06872446 0.94721997]] real [[0.91062237 0.06550179]\n",
      " [0.06550179 0.95199599]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.91117018 0.06875001]\n",
      " [0.06510032 0.94961548]] real [[0.91472828 0.06249271]\n",
      " [0.06249271 0.95420124]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.66532452 0.2998643 ]\n",
      " [0.24527215 0.78023977]] real [[0.71301692 0.21032003]\n",
      " [0.21032003 0.84586368]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.85493476 0.11670119]\n",
      " [0.10631332 0.91447371]] real [[0.86404167 0.09963919]\n",
      " [0.09963919 0.92697787]]\n",
      "features (2, 2) omega matrix (2, 2) both (2, 2)\n",
      "test [[0.81660402 0.17076934]\n",
      " [0.13440461 0.87484903]] real [[0.82271589 0.12992543]\n",
      " [0.12992543 0.90478212]]\n",
      "features (3, 2) omega matrix (3, 3) both (2, 3)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (3,3) ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m contexts_generator \u001b[38;5;241m=\u001b[39m synthetic_data\u001b[38;5;241m.\u001b[39mLinearContexts( np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m]), \u001b[38;5;241m0\u001b[39m, d, margin) \u001b[38;5;66;03m#synthetic_data.ToyContexts( ) #\u001b[39;00m\n\u001b[1;32m     21\u001b[0m job \u001b[38;5;241m=\u001b[39m (outcome_distribution, contexts_generator, \u001b[38;5;241m3\u001b[39m )\n\u001b[0;32m---> 23\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_policy_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43malg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mEvaluation.eval_policy_once\u001b[0;34m(self, alg, game, job)\u001b[0m\n\u001b[1;32m     96\u001b[0m context \u001b[38;5;241m=\u001b[39m contexts[t]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# print(context.T.shape)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# policy chooses one action\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# print('t', t,  'outcome', outcome, 'context', context)\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43malg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# print('t', t, 'action', action, 'outcome', outcome, 'context', context)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m feedback \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_feedback( game, action, outcome )\n",
      "File \u001b[0;32m~/Desktop/attack-detection/partial_monitoring/PGIDSratio.py:67\u001b[0m, in \u001b[0;36mPGIDSratio.get_action\u001b[0;34m(self, t, X)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpmean\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Gibbs sampling\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthetasamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthetagibbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthetasamples[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# print('samples', self.thetasamples)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Compute gap estimates\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/attack-detection/partial_monitoring/PGIDSratio.py:40\u001b[0m, in \u001b[0;36mPGIDSratio.thetagibbs\u001b[0;34m(self, contexts, outcomes)\u001b[0m\n\u001b[1;32m     38\u001b[0m test \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m Omegamat\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124momega matrix\u001b[39m\u001b[38;5;124m'\u001b[39m, Omegamat\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m, test\u001b[38;5;241m.\u001b[39mshape  )\n\u001b[0;32m---> 40\u001b[0m Vomega_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpcovar_inv \u001b[38;5;241m-\u001b[39m \u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpcovar_inv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpcovar_inv\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpcovar_inv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m Vomega   \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(  features\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m Omegamat \u001b[38;5;241m@\u001b[39m features \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpcovar ) \u001b[38;5;66;03m#variance\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, Vomega_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m, Vomega )\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (3,3) "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import PM_DMED\n",
    "\n",
    "n_cores = 1\n",
    "n_folds = 1\n",
    "horizon = 100\n",
    "\n",
    "# np.seterr(all='raise')\n",
    "\n",
    "# game = games.apple_tasting(False, outcome_distribution) \n",
    "\n",
    "outcome_distribution = [0.8,0.2]\n",
    "job = (outcome_distribution, 1 )\n",
    "\n",
    "\n",
    "game =  games.label_efficient(  ) \n",
    "game.set_outcome_distribution( {'spam':outcome_distribution[0],'ham':outcome_distribution[1]} )\n",
    "print('optimal action', game.i_star)\n",
    "\n",
    "\n",
    "# print('optimal action', game.i_star)\n",
    "alg = cpb.CPB(  game, horizon,1.01) #TSPM.TSPM_alg(  game, horizon, 1)\n",
    "task = Evaluation(horizon, 'easy')\n",
    "\n",
    "result = task.eval_policy_once(alg,game, job)\n",
    "#plt.plot(range(horizon), result)\n",
    "# fig = go.Figure( )\n",
    "# regret = np.array([ game.delta(i) for i in range(game.n_actions) ]).T @ np.mean(result,0) \n",
    "# xcoords = np.arange(0,horizon,1).tolist()\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='blue'), mode='lines',  name='TPSM' )) # \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_cores = 8\n",
    "n_folds = 50\n",
    "horizon = 10000\n",
    "# outcome_distribution =  {'spam':0.05,'ham':0.95}\n",
    "\n",
    "game = games.label_efficient(  )\n",
    " \n",
    "algos = [ PGIDSratio.PGIDSratio( game, horizon, 2 )   ]\n",
    "\n",
    "\n",
    "colors = [  [0,0,0], [250,0,0], [200,0,0], [0,200,0]   ] #[0,150,0], [0,250,0], [0,150,0], [0,0,250], [0,0,0],  [0,255,0], [0 , 150, 0], [155,155,0], [255,0,0], [0,0,255] , [255,51,255], [255,51,255], [255,20,200]  ] #\n",
    "labels = [  'random',  'CPB',  'RandCPB', 'TSPM'   ]  # 'random', 'TSPM' , 'TSPM (R=0)', 'RandCBP (uncoupled)','CPB uniform', ,'TSPM' , 'ucbTSPM (Auer)' 'FeedExp3 (2001)', 'FeedExp3 (2006)', 'CPB',  'eTSPM (Auer)',\n",
    "\n",
    "fig = go.Figure( )\n",
    "\n",
    "final_regrets = []\n",
    "\n",
    "for alg, color, label in zip( algos, colors, labels):\n",
    "\n",
    "    r,g,b = color\n",
    "    result = evaluate_parallel(n_cores, n_folds, horizon, alg, game, 'harsch')\n",
    "    final_regrets.append( result[:,-1] )\n",
    "    regret =  np.mean(result, 0) \n",
    "    xcoords = np.arange(0,horizon,1).tolist()\n",
    "    std =  np.std(result,0) \n",
    "    upper_regret = regret + std\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='rgb({},{},{})'.format(r,g,b)), mode='lines',  name=label )) # \n",
    "\n",
    "    fig.add_trace(   go.Scatter( x=xcoords+xcoords[::-1], y=upper_regret.tolist()+regret.tolist()[::-1],  fill='toself', fillcolor='rgba({},{},{},0.2)'.format(r,g,b), \n",
    "                         line=dict(color='rgba(255,255,255,0)'),   hoverinfo=\"skip\",  showlegend=False )  )\n",
    "    \n",
    "fig.show(legend=True)\n",
    "fig.update_yaxes(range=[0, 30] )\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                  xaxis_title=\"Sequence\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "fig.write_image(\"./hard_LE.pdf\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "final_regrets = np.array(final_regrets)\n",
    "final = [ ( np.argmin(final_regrets[:,i]), i) for i in range(n_folds) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig.update_xaxes(type=\"linear\")\n",
    "fig.update_yaxes(range=[0, 12000] )\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                  xaxis_title=\"Sequence\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "fig.write_image(\"./hard_LE_log.pdf\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_cores = 8\n",
    "n_folds = 200\n",
    "horizon = 5000\n",
    "# outcome_distribution =  {'spam':0.05,'ham':0.95}\n",
    "\n",
    "game = games.label_efficient(  )\n",
    "\n",
    "#feedexp3.FeedExp3(  game, horizon, ),\n",
    "#feedexp3_v3.FeedExp3(  game, horizon, ),\n",
    "#eTSPM.eTSPM_alg(  game, horizon, 1),\n",
    "#TSPM.TSPM_alg(  game, horizon,), bpm.BPM(  game, horizon,  [0.5, 0.5 ], np.identity(2) ) ,\n",
    "\n",
    "#TSPM.TSPM_alg(  game, horizon,), \n",
    "#ucbTSPM_v2.TSPM_alg(game, horizon)  \n",
    "\n",
    "algos = [ random_algo.Random(  game, horizon, ),      \n",
    "        cpb.CPB(  game, horizon, 1.01), \n",
    "        cpb_gaussian.CPB_gaussian(  game, horizon, 1.01, True, 1/16, 10),\n",
    "        TSPM.TSPM_alg(  game, horizon, 1),   ] \n",
    "\n",
    "        #cpb_gaussian_v2.CPB_gaussian(  game, horizon, 1.01, True), \n",
    "        #cpb_gaussian_v2.CPB_gaussian(  game, horizon, 1.01, False), ]\n",
    "        #cpb_gaussian_v2.CPB_gaussian(  game, horizon, 1.01),\n",
    "        #TSPM.TSPM_alg(game, horizon, 1 ) ]\n",
    "        #TSPM.TSPM_alg(game, horizon, 0 )   #eTSPM.eTSPM_alg(game, horizon, 1), cpb_uniform.CPB_uniform(  game, horizon, 1.01), \n",
    "\n",
    "colors = [  [0,0,0], [250,0,0], [200,0,0], [0,200,0]   ] #[0,150,0], [0,250,0], [0,150,0], [0,0,250], [0,0,0],  [0,255,0], [0 , 150, 0], [155,155,0], [255,0,0], [0,0,255] , [255,51,255], [255,51,255], [255,20,200]  ] #\n",
    "labels = [  'random',  'CPB',  'RandCPB', 'TSPM'   ]  # 'random', 'TSPM' , 'TSPM (R=0)', 'RandCBP (uncoupled)','CPB uniform', ,'TSPM' , 'ucbTSPM (Auer)' 'FeedExp3 (2001)', 'FeedExp3 (2006)', 'CPB',  'eTSPM (Auer)',\n",
    "\n",
    "fig = go.Figure( )\n",
    "\n",
    "final_regrets = []\n",
    "\n",
    "for alg, color, label in zip( algos, colors, labels):\n",
    "\n",
    "    r,g,b = color\n",
    "    result = evaluate_parallel(n_cores, n_folds, horizon, alg, game, 'easy')\n",
    "    final_regrets.append( result[:,-1] )\n",
    "    regret =  np.mean(result, 0) \n",
    "    xcoords = np.arange(0,horizon,1).tolist()\n",
    "    std =  np.std(result,0) \n",
    "    upper_regret = regret + std\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=xcoords, y=regret, line=dict(color='rgb({},{},{})'.format(r,g,b)), mode='lines',  name=label )) # \n",
    "\n",
    "    fig.add_trace(   go.Scatter( x=xcoords+xcoords[::-1], y=upper_regret.tolist()+regret.tolist()[::-1],  fill='toself', fillcolor='rgba({},{},{},0.2)'.format(r,g,b), \n",
    "                         line=dict(color='rgba(255,255,255,0)'),   hoverinfo=\"skip\",  showlegend=False )  )\n",
    "    \n",
    "fig.show(legend=True)\n",
    "fig.update_yaxes(range=[0, 30] )\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                  xaxis_title=\"Sequence\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "fig.write_image(\"./easy_LE.pdf\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "final_regrets = np.array(final_regrets)\n",
    "final = [ ( np.argmin(final_regrets[:,i]), i) for i in range(n_folds) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "res = []\n",
    "for i in range(n_folds):\n",
    "    opt = min(final_regrets[:,i])\n",
    "    vec = [ j == opt for j in final_regrets[:,i] ] \n",
    "    #if vec[1]==1 and vec[2] == 1:\n",
    "    #    pass\n",
    "    #else:\n",
    "    res.append(  vec  )\n",
    "\n",
    "#print( np.sum( res,0) )\n",
    "\n",
    "diff = []\n",
    "for i in range(n_folds):\n",
    "    if res[i][1] >= res[i][2]:\n",
    "        diff.append( i )\n",
    "\n",
    "np.random.seed(1)\n",
    "distributions = []\n",
    "for jobid in range(n_folds):\n",
    "    p = np.random.uniform(0.4, 0.5) \n",
    "    distributions.append( [p, 1-p] )\n",
    "\n",
    "distributions_rand = np.array([ distributions[i] for i in diff ])\n",
    "distributions_cbp = np.array([ distributions[i] for i in range(n_folds) if i not in diff ])\n",
    "#print( len( diff ) )\n",
    "print(np.mean(distributions_rand[:,0]), np.std(distributions_rand[:,0]) )\n",
    "print(np.mean(distributions_cbp[:,0]), np.std(distributions_cbp[:,0]) )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.456154090331781 0.03132638699822154\n",
      "0.43466197585846006 0.024706747598054565\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_layout(legend= dict(yanchor=\"top\",y=0.98,xanchor=\"left\",x=0.77), autosize=False,\n",
    "                  xaxis_title=\"Sequence (log-scale)\", yaxis_title=\"Regret\",  margin=go.layout.Margin( l=0,   r=0,   b=0,    t=0, ),   font=dict(size=13,) )\n",
    "fig.write_image(\"./easy_LE_log.pdf\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "fig.write_image(\"./easy_LE.pdf\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0215ed8d73c40df5de54d647d65b604dcf02460a7de2b27ed9878602c67cf72c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.15 64-bit ('climate-ai': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}